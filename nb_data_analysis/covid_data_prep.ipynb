{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# COVID-19 data\n",
    "\n",
    "- Notes of patients with the diagnosis `COVID-19, virus geÃ¯dentificeerd \\[U07.1]`.\n",
    "- The data is from 2020 and Q1 of 2021.\n",
    "- The data is from the two locations of the Amsterdam UMC: `amc` and `vumc`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "from pathlib import Path\n",
    "from utils.config import PATHS\n",
    "from utils.data_process import concat_annotated, fix_week_14, anonymize"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "datapath = PATHS.getpath('data')\n",
    "\n",
    "cov1 = pd.read_pickle(datapath / '2020_raw/ICD_U07.1/notes_[U07.1]_2020_q1_q2_q3.pkl')\n",
    "cov2 = pd.read_pickle(datapath / '2020-Q4_2021-Q1_raw/ICD_U07.1/notes_[U07.1]_2020_q4_2021_q1.pkl')\n",
    "\n",
    "df = pd.concat([cov1, cov2], ignore_index=True).drop_duplicates(subset=['MDN', 'NotitieID', 'all_text'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mark annotated"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def isin_multicol(\n",
    "    df1 : pd.DataFrame,\n",
    "    df2 : pd.DataFrame,\n",
    "    *args\n",
    ") -> pd.Series:\n",
    "    cols = list(args)\n",
    "    return df1.set_index(cols).index.isin(df2.set_index(cols).index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "annotated = pd.read_csv(datapath / 'annotated_notes_ids.csv', dtype={'MDN': str, 'NotitieID': str})\n",
    "\n",
    "df['annotated'] = df.pipe(isin_multicol, annotated, 'institution', 'MDN', 'NotitieID')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process annotations and add gold labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "datapath = PATHS.getpath('data_expr_sept')\n",
    "\n",
    "domains=['ADM', 'ATT', 'BER', 'ENR', 'ETN', 'FAC', 'INS', 'MBW', 'STM']\n",
    "levels = [f\"{domain}_lvl\" for domain in domains]\n",
    "info_cols = ['institution', 'MDN']\n",
    "other = ['target', 'background', 'plus']\n",
    "\n",
    "def add_boolean_selectors(df):\n",
    "    is_background = lambda df: df.groupby('sen_id').background.transform('any')\n",
    "    is_target = lambda df: df.groupby('sen_id').target.transform('any')\n",
    "    is_disregard = lambda df: df.groupby('NotitieID').disregard.transform('any')\n",
    "    return df.assign(\n",
    "        background_sent = is_background,\n",
    "        target_sent = is_target,\n",
    "        disregard_note = is_disregard,\n",
    "    )\n",
    "\n",
    "def standardize_nans(df):\n",
    "    df[domains + other] = df[domains + other].fillna(False)\n",
    "    df[['label', 'relation']] = df[['label', 'relation']].fillna('_')\n",
    "    df['token'] = df['token'].fillna('')\n",
    "    return df\n",
    "\n",
    "def load_and_preprocess_data(datapath):\n",
    "    return concat_annotated(datapath\n",
    "        ).pipe(fix_week_14\n",
    "        ).pipe(add_boolean_selectors\n",
    "        ).pipe(standardize_nans)\n",
    "\n",
    "def create_note_level_labels(df, info_cols, levels):\n",
    "    info = df.groupby('NotitieID')[info_cols].first()\n",
    "    labels = df.groupby('NotitieID')[levels].mean()\n",
    "    return pd.concat([info, labels], axis=1).reset_index()\n",
    "\n",
    "annot = load_and_preprocess_data(datapath)\n",
    "gold = create_note_level_labels(annot, info_cols, levels)\n",
    "\n",
    "df = df.merge(gold, how='left', on=['NotitieID', 'MDN', 'institution'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare for ML pipeline\n",
    "\n",
    "The notes for which there are no gold labels, get labeled by the ML models. Since this is done on a separate server, the actual functions are not in this notebook; here only the inputs for ML are prepared and the outputs from ML are processed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nlp = spacy.load('nl_core_news_lg')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Anonymize notes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "datapath = PATHS.getpath('covid_data')\n",
    "\n",
    "to_ml = df.query(\"not annotated\").all_text\n",
    "notes = to_ml.apply(lambda i: anonymize(i, nlp)[0])\n",
    "# notes.to_pickle(datapath / 'intermediate_files/anonym_notes.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split to sentences"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "to_sentence = lambda i: [str(t) for t in list(nlp(i).sents)]\n",
    "sents = notes.apply(to_sentence).explode().rename('text').reset_index()\n",
    "# sents.to_pickle(datapath / 'intermediate_files/sents.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# the sentences are split into batches of 200,000 sentences\n",
    "# the batches are sent to the domain classifier\n",
    "\n",
    "sents = sents.assign(chunk = lambda df: df.index // 200000)\n",
    "\n",
    "for chunk in sents.chunk.unique():\n",
    "    if chunk == 0:\n",
    "        continue\n",
    "    boolean = sents.chunk == chunk\n",
    "    sents.loc[boolean, :'text'].to_pickle(f\"sents_part{chunk}.pkl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process domain predictions for level clf's"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# this file is the concatination of the labeled batches\n",
    "\n",
    "dom_preds = pd.read_pickle(datapath / 'intermediate_files/sents_with_dom_preds.pkl')\n",
    "pred_col = 'pred_domains_eb_ap_mod1'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# for each domain, sentences that were labeled as this domain are sent to the level classifier\n",
    "\n",
    "for i, dom in enumerate(domains):\n",
    "    boolean = dom_preds[pred_col].apply(lambda x: bool(x[i]))\n",
    "    results = dom_preds[boolean]\n",
    "    if results.empty:\n",
    "        print(f\"we have a boner: {dom}!\")\n",
    "        continue\n",
    "    print(f\"{dom}: {len(results)=}\")\n",
    "    results.to_pickle(f\"lvl_preds_{dom}.pkl\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ADM: len(results)=107421\n",
      "ATT: len(results)=1047\n",
      "BER: len(results)=2212\n",
      "ENR: len(results)=10671\n",
      "ETN: len(results)=31783\n",
      "FAC: len(results)=21729\n",
      "INS: len(results)=9794\n",
      "MBW: len(results)=5292\n",
      "STM: len(results)=20165\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process level predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "dom_lvl_preds = dom_preds.copy()\n",
    "for f in Path().glob('lvl_preds_*.pkl'):\n",
    "    col = f\"{f.stem[-3:]}_lvl\"\n",
    "    dom_lvl_preds = dom_lvl_preds.merge(\n",
    "        pd.read_pickle(f)[col],\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Aggregate to note-level"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "predictions = dom_lvl_preds.groupby('index')[levels].mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merge back to the original df"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "df = df.loc[~df.annotated, :'annotated'].merge(\n",
    "    predictions,\n",
    "    how='left',\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").append(df.loc[df.annotated]).sort_index()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# df.drop(columns='all_text').to_pickle(datapath / 'covid_data.pkl')\n",
    "# df.drop(columns='all_text').to_csv(datapath / 'covid_data.tsv', sep='\\t', index=False)\n",
    "\n",
    "# df.to_pickle(datapath / 'covid_data_with_text.pkl')\n",
    "# df.to_csv(datapath / 'covid_data_with_text.tsv', sep='\\t', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('zonmw': conda)"
  },
  "interpreter": {
   "hash": "5a0a696a2c4562c805ef24bd77b26c70704f51e5d276869b3745fd7123bf8c36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}