Evaluation of the models
=======================
# Domains
The notebook [clf_domains_eval.ipynb](clf_domains_eval.ipynb) processes a pickled dataframe with gold labels and predictions and generates the following metrics:
- Precision, recall, F1-score per domain: on sentence-level and aggregated to note-level.
- Confusion matrix (sentence-level).

The notebook can also be used to select examples for error analysis.

The predictions are generated with the [predict.py](../clf_domains/predict.py) script.

# Levels
The script [clf_levels_eval_note_agg.py](clf_levels_eval_note_agg.py) processes a pickled dataframe with sentence-level gold labels and predictions to generate evaluation metrics on a **note-level**. The metrics include:
- mean absolute error (MAE)
- mean squared error (MSE)
- root mean squared error (RMSE)

The predictions are generated with the [predict.py](../clf_levels/predict.py) script.

The metrics on a sentence-level are generated by the [evaluate_model.py](../clf_levels/evaluate_model.py) script and can be found in the models' directories.
